{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ⭐\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth#installation-instructions---conda).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "**[NEW] Llama-3 8b is trained on a crazy 15 trillion tokens! Llama-2 was 2 trillion.**\n",
        "\n",
        "Use our [Llama-3 8b Instruct](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing) notebook for conversational style finetunes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "%pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "# !pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "%pip install -U wandb\n",
        "# !pip install -U wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* With [PR 26037](https://github.com/huggingface/transformers/pull/26037), we support downloading 4bit models **4x faster**! [Our repo](https://huggingface.co/unsloth) has Llama, Mistral 4bit models.\n",
        "* [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L1X-Hh-eMb2",
        "outputId": "63aa4a1c-4611-4c1d-db6e-79dec35b2501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get(\"HUGGINGFACE_TOKEN\")\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "mi4sozuWf05e",
        "outputId": "0d5474ff-4fc3-47cb-bc5a-bff214948cf5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchukypedro15\u001b[0m (\u001b[33mchukypedro\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240719_143208-45jn2tue</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chukypedro/software-automation-ai_unsloth/runs/45jn2tue' target=\"_blank\">dauntless-energy-1</a></strong> to <a href='https://wandb.ai/chukypedro/software-automation-ai_unsloth' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/chukypedro/software-automation-ai_unsloth' target=\"_blank\">https://wandb.ai/chukypedro/software-automation-ai_unsloth</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/chukypedro/software-automation-ai_unsloth/runs/45jn2tue' target=\"_blank\">https://wandb.ai/chukypedro/software-automation-ai_unsloth/runs/45jn2tue</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)\n",
        "run = wandb.init(\n",
        "    project='software-automation-ai_unsloth',\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330,
          "referenced_widgets": [
            "09044b4f54054eedb09bc469b8a90cf1",
            "509aee3ff9f748228091ee94dd340a52",
            "f060ffe3b4e94f46ab6a32b39caf3b82",
            "97b54e6d916742ed9648b485858ad21f",
            "413309d100c7448591ee438f77d9e705",
            "6b2983ff29804c82b2a2c33673030bda",
            "04ed7f3609e44cf1a1ac1e20037a520f",
            "1c72734232014f8281fe5c6489dbdec7",
            "5f6e6f69ba44454e8f891e885611b3f9",
            "bbb753733e3f4aed9d73beb25084bedf",
            "00ecc999da0d44a6ac1094adabd31c6b",
            "3e0abf68ef1c424291268a30155cb6fa",
            "97037f8b83d14d848fd34cabb064bbfc",
            "808dc110997e4f83937edc38a143e69a",
            "df03e34db2f54119b307180123765b71",
            "36624764402e4fe1adcc8656c8e25038",
            "7b497abd5a164163928b1e83ebed2951",
            "4b8e1d8a715f4d2b8ab0734f4f90a00e",
            "14f4435713044a9ba9ec38e1a588ef6b",
            "24a2aff08a0142a3b91c0be3fc1bb4b2",
            "583b91cac2444692b41f2c515d592345",
            "fbd405f0c30b46a89c4d46baca78bfa4",
            "23201ef69b6d42099ab722ade8f2e852",
            "486d45c7168b414eaec85426be564f7c",
            "231979b0d03c471fb763e737eca12093",
            "d916993dcf6347379221a523eae5104c",
            "d4fb4d6f05314372a41431013f7d0383",
            "80af76274b454071953adf69b0286079",
            "bfa39246b2be4370b911a6ee5e72e118",
            "d837e42e687f479a94bebfd10dcd42c5",
            "e43752904c8240a5a86f8744ee4ce025",
            "57c19b5efd0a4678b0fca61aa3fd84b5",
            "ccd425a2a6ff401e873a390b00236bde",
            "bf114227cadd40b3900c1fe60cc1267f",
            "4eba10188f2e4481b7cb77b7a6303684",
            "2349e817f085499788cb7c3e41ef4844",
            "c69695fb56af447bb3bee555b2b4d6a5",
            "885968190880411bb6d22053a5b27f35",
            "470796be52464966b9fc98cc6fc88d25",
            "1bd98275af5040a5b8bed7651def4fa5",
            "4fb476d1c2a643c483687208175ee456",
            "53a189a985204b9ea737eacdd9bd4288",
            "7df328b0855643bb9a36799182718ed9",
            "a4552b8978e24db8b0978b767b6f325e",
            "886c8af9ece3489d92b8e6439e69bd23",
            "ed943cafa948435a95c10da5182c60f1",
            "bcc7501bd0ef4733bcf4821a507b66ff",
            "43c84c1537984cfca9c5c48c0a078c07",
            "8462a517eb5d4772907eb3c58cf30feb",
            "9c4fc117a8374cb4abbe8233394cefd9",
            "ee9e3a1020544c54bd6888f0179bbc86",
            "72c19ba142334e31890e0959412f0432",
            "78d1883404394feca2a1b746a1179d3e",
            "194f8890bf234c78970e31a5f4c455ba",
            "389481a1e6e744909196bb4642bfb91a",
            "be7964a39d0a4c0789e06f19faf5467f",
            "530f7170009a47129ff384a7d6ef1034",
            "195ac429386240e49f9e56a601f61cf9",
            "d5c10faebb2f44b6bd1431a11ae0e927",
            "af38fd17e41448ada0e42026dc54992f",
            "450388b09b4340ffb4228366eedb9669",
            "1fbb4b9e325241009972bbaa4a788c92",
            "8348574c0088456e878d70f3389b3f45",
            "db349839b0e14b03aca8fce24b40eb72",
            "953886005da34a4387eaec8fbc55c475",
            "28230f6db2244ef5a457b520429f83d5"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "3974932e-11df-41c8-c150-90032c160652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "09044b4f54054eedb09bc469b8a90cf1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
            "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.564 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.3.1+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e0abf68ef1c424291268a30155cb6fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23201ef69b6d42099ab722ade8f2e852",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf114227cadd40b3900c1fe60cc1267f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "886c8af9ece3489d92b8e6439e69bd23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be7964a39d0a4c0789e06f19faf5467f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/464 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "55ff75d8-3ebe-4bda-b120-77076690eac5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "30aadb310e6047d6a89a27231c54c317",
            "a8a64aff44ed4d97821e168014c39226",
            "32b58b96f6b34ea1a8e71684fa6f6de5",
            "fb171cd945294e11ab7746f4f188b580",
            "9d04661880b547e6a37ad1ad3c31e9f4",
            "c62907ab4a604f08b52c62d2f44b09ab",
            "07c05e64f00c4b7ba2524b3d791f9a76",
            "a0f37ecb851f42fe85a3a84709e78e80",
            "114b6248ab434e6097894a66f54cae71",
            "996595a5c8e746f895e76b76f34c16e3",
            "f322530d67cd441a91639900918b6d3a",
            "2466e42ae8cf431a9fd8606c2a62e2a4",
            "337069e1a13849db8073796f372781cd",
            "d8ae6010d2954d58bbe4b1de7998322d",
            "53d52987cf77436fad96e74a9b22f4bb",
            "03c196304147433ebb5f3c58550059f3",
            "5af4083475674845a1daea699508cd7f",
            "df9f4b1ce9984ac5b45157039873299e",
            "a72390e0cd604ae8bd222842e441435a",
            "79d9e31ccc384903bbf1b8b8bc3af583",
            "ffc851c5002e4690ad3b7362e776b371",
            "8f954e42c4074441a3ec1d0f9393c995",
            "3666db114441410fb66363d64b6ea24f",
            "0964570818894eda983113ef33671291",
            "8ac28dac0a1241dcabedf19bf0101d1d",
            "3b443b6c1ec14824a6aeec2e195fb2c0",
            "b3ba41c4fa184c9e8ab3f33f4e01ff88",
            "14c6448b18d143ad91990a74f95510cb",
            "2cd3bd2e23564d86ae7110458d064146",
            "2f81d60fa29e4280bbf5315c69151d30",
            "9b57877d0bb14f03806a145f7ec67a2a",
            "1ac3231910f445328d478b2ad358c79b",
            "7c48676eaa1246068486a6f1326ccc3a",
            "9e2317b56f3f48d987205b605bf8976c",
            "2c402ad9763645adb687f4a57e828e97",
            "477c22f48991468da5d23f5253cdaa6c",
            "7c279da6d42a4de789e8e47c67e9bd64",
            "98584130cbf8481da7696706ee3851ed",
            "06b026e54c4f47d8b83aafbec2247d1c",
            "775160414acd471589da1667db3499e7",
            "72d4e610999e4e16b8564fc483c9a4d6",
            "835045ed12b44e53909ba8453aa7f624",
            "3c810fd75e4946b483d9e7ffbbfd0d78",
            "2c04c96a30d74f6aa67c8ac1092d9eca"
          ]
        },
        "id": "g9FeLJY3Kma-",
        "outputId": "23d4b2b6-6ce5-4af0-849a-b32218294f7e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30aadb310e6047d6a89a27231c54c317",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2466e42ae8cf431a9fd8606c2a62e2a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/3.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3666db114441410fb66363d64b6ea24f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/1069 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e2317b56f3f48d987205b605bf8976c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1069 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"chukypedro/robotframework_dataset\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpxR3BRa4ksh",
        "outputId": "addd92ee-e982-4717-9deb-30ee2be6951c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'output', 'input', 'text'],\n",
              "        num_rows: 962\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'output', 'input', 'text'],\n",
              "        num_rows: 107\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weEf9I-Y1_HM",
        "outputId": "6f34a2f1-b8e6-4ad9-e46f-524c4213506b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate data-driven clientcentral_testcases for orders Testcases TC_45 Validate Merchant Can View Order History\n",
            "\n",
            "### Input:\n",
            "Test Case Type: data-driven\n",
            " Test Case Name: TC_45 Validate Merchant Can View Order History\n",
            "\n",
            "### Response:\n",
            "*** Settings ***\n",
            "Documentation    Validate TC_45 Validate Merchant Can View Order History\n",
            "Test Template    Test Execution\n",
            "Resource         resource_file.robot\n",
            "\n",
            "*** Variables ***\n",
            "${VALID_USER}       testuser\n",
            "${VALID_PASSWORD}   secret\n",
            "\n",
            "*** Test Cases ***\n",
            "TC_45 Validate Merchant Can View Order History\n",
            "[Documentation]    Validate TC_45 Validate Merchant Can View Order History using the following steps\n",
            "Select Module    ${Orders}    ${}    ${}\n",
            "Navigate To Order History Page    ${month}    ${}    ${}\n",
            "Validate Order Entries Are Displayed    ${order_id}    ${}    ${}\n",
            "Filter Orders By Status    ${Completed}    ${}    ${}\n",
            "Validate Filtered Orders    ${order_id}    ${Completed}    ${}\n",
            "\n",
            "*** Keywords ***\n",
            "Select Module\n",
            "[Arguments]    ${Orders}    ${}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${Orders}    ${}    ${}\n",
            "\n",
            "Navigate To Order History Page\n",
            "[Arguments]    ${month}    ${}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${month}    ${}    ${}\n",
            "\n",
            "Validate Order Entries Are Displayed\n",
            "[Arguments]    ${order_id}    ${}    ${}\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${order_id}    ${}    ${}\n",
            "\n",
            "Filter Orders By Status\n",
            "[Arguments]    ${Completed}    ${}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${Completed}    ${}    ${}\n",
            "\n",
            "Validate Filtered Orders\n",
            "[Arguments]    ${order_id}    ${Completed}    ${}\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${order_id}    ${Completed}    ${}\n",
            "<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "print(dataset['test']['text'][5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgYibV3a101D"
      },
      "outputs": [],
      "source": [
        "# dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d9b12a9b89a643f7a721b086aba667c7",
            "eb7dbc75f73e48f693662da97e5ff7f3",
            "5ea2823510574579b28aaf17250c6a70",
            "3d6c58aea24649ec9deb416e93731bd9",
            "4a06cf3bf52e490c8ff78a5ed0b5af8d",
            "384b85c6341747cb8b5b4d71362dd0f2",
            "58f6255e2fae4f589c2c8dcc49cf2468",
            "b7530a2f26b64f3489fd10bf15bc2139",
            "fcc224472ca241098a7eee0bb93c0f10",
            "31be97e54c2e45368f0798a5fcb86e3d",
            "01ce1b9cf5ac4e4fadcfa849d87032dc",
            "21d1a908a479406ca1ac163ded0bb487",
            "d955f14d407f4d3686ab7dd1bcdc7ed7",
            "6cf1a8b6d8754363aca6cd880ce79188",
            "6581633425ff46bfbc76424e91f2bad3",
            "8954dafd688e47ab82e4c1f94ab68e6c",
            "486bd0ff79514cff913afab0e37049a2",
            "0a79ff85146f437aa1b825e39c772135",
            "e5a6d9d60705412abb332dd8d14e506f",
            "ac14d1761ed847b8a414e456a66ab09e",
            "3500406046e6400daefebd21cf9d15ae",
            "45ac9f3fc0784d41b67afa5d1386835a"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "47533c48-3768-4e9a-db84-98901867cd31"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9b12a9b89a643f7a721b086aba667c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/962 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "21d1a908a479406ca1ac163ded0bb487",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/107 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        eval_strategy=\"steps\",  # Updated key here\n",
        "        eval_steps=20,\n",
        "        logging_steps = 20,\n",
        "        num_train_epochs = 10,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"wandb\",\n",
        "\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "034cf186-7722-4739-c828-13347dfce125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.168 GB.\n",
            "5.594 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "f67da23e-d4c4-46e7-a0cc-24597ab4262b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 962 | Num Epochs = 10\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 1,200\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='821' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 821/1200 41:10 < 19:03, 0.33 it/s, Epoch 6.82/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.617000</td>\n",
              "      <td>0.261096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.228600</td>\n",
              "      <td>0.194582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.184300</td>\n",
              "      <td>0.175145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.164200</td>\n",
              "      <td>0.162409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.155900</td>\n",
              "      <td>0.157610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.177800</td>\n",
              "      <td>0.152516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.139900</td>\n",
              "      <td>0.148697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.134200</td>\n",
              "      <td>0.147587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.132200</td>\n",
              "      <td>0.142068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.132800</td>\n",
              "      <td>0.140915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.138509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.134505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.106900</td>\n",
              "      <td>0.136379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.098100</td>\n",
              "      <td>0.136073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>0.134642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.134496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.110500</td>\n",
              "      <td>0.135119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.098300</td>\n",
              "      <td>0.132120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.077000</td>\n",
              "      <td>0.138352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.075500</td>\n",
              "      <td>0.144987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.141522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.084400</td>\n",
              "      <td>0.140552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.076800</td>\n",
              "      <td>0.140294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.089200</td>\n",
              "      <td>0.136647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.058500</td>\n",
              "      <td>0.148758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.058000</td>\n",
              "      <td>0.152224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.062800</td>\n",
              "      <td>0.149416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.150082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>0.148621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.156669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.043300</td>\n",
              "      <td>0.162066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.163021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.042600</td>\n",
              "      <td>0.167578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.045100</td>\n",
              "      <td>0.164187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.163037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.046000</td>\n",
              "      <td>0.160673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>0.175437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.031500</td>\n",
              "      <td>0.181499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.033400</td>\n",
              "      <td>0.173536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>0.176582</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5/14 00:02 < 00:06, 1.43 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1200 1:00:31, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.617000</td>\n",
              "      <td>0.261096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.228600</td>\n",
              "      <td>0.194582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.184300</td>\n",
              "      <td>0.175145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.164200</td>\n",
              "      <td>0.162409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.155900</td>\n",
              "      <td>0.157610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.177800</td>\n",
              "      <td>0.152516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.139900</td>\n",
              "      <td>0.148697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.134200</td>\n",
              "      <td>0.147587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.132200</td>\n",
              "      <td>0.142068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.132800</td>\n",
              "      <td>0.140915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.135400</td>\n",
              "      <td>0.138509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.134505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.106900</td>\n",
              "      <td>0.136379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.098100</td>\n",
              "      <td>0.136073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.107500</td>\n",
              "      <td>0.134642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.134496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.110500</td>\n",
              "      <td>0.135119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.098300</td>\n",
              "      <td>0.132120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.077000</td>\n",
              "      <td>0.138352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.075500</td>\n",
              "      <td>0.144987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.080500</td>\n",
              "      <td>0.141522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.084400</td>\n",
              "      <td>0.140552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.076800</td>\n",
              "      <td>0.140294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.089200</td>\n",
              "      <td>0.136647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.058500</td>\n",
              "      <td>0.148758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.058000</td>\n",
              "      <td>0.152224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.062800</td>\n",
              "      <td>0.149416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.061900</td>\n",
              "      <td>0.150082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.059000</td>\n",
              "      <td>0.148621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.059700</td>\n",
              "      <td>0.156669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.043300</td>\n",
              "      <td>0.162066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.163021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.042600</td>\n",
              "      <td>0.167578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.045100</td>\n",
              "      <td>0.164187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.163037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.046000</td>\n",
              "      <td>0.160673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>0.175437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.031500</td>\n",
              "      <td>0.181499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.033400</td>\n",
              "      <td>0.173536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.032900</td>\n",
              "      <td>0.176582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>0.179576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.034600</td>\n",
              "      <td>0.180389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.025700</td>\n",
              "      <td>0.198733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.024300</td>\n",
              "      <td>0.198120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.025400</td>\n",
              "      <td>0.192467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.027100</td>\n",
              "      <td>0.195072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.026300</td>\n",
              "      <td>0.194210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>0.027100</td>\n",
              "      <td>0.195339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>0.021000</td>\n",
              "      <td>0.213086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.020600</td>\n",
              "      <td>0.215863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.215380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>0.020100</td>\n",
              "      <td>0.219050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>0.020100</td>\n",
              "      <td>0.216312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>0.021800</td>\n",
              "      <td>0.216999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.018400</td>\n",
              "      <td>0.230706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>0.017300</td>\n",
              "      <td>0.238564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>0.016900</td>\n",
              "      <td>0.237777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.236837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.238326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.018300</td>\n",
              "      <td>0.239294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBeyOIvSwqgU",
        "outputId": "8c8a7d0a-14eb-4d28-e26a-04723ca8000f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST EXAMPLE:  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate data-driven clientcentral_testcases for orders Testcases TC_45 Validate Merchant Can View Order History\n",
            "\n",
            "### Input:\n",
            "Test Case Type: data-driven\n",
            " Test Case Name: TC_45 Validate Merchant Can View Order History\n",
            "\n",
            "### Response:\n",
            "*** Settings ***\n",
            "Documentation    Validate TC_45 Validate Merchant Can View Order History\n",
            "Test Template    Test Execution\n",
            "Resource         resource_file.robot\n",
            "\n",
            "*** Variables ***\n",
            "${VALID_USER}       testuser\n",
            "${VALID_PASSWORD}   secret\n",
            "\n",
            "*** Test Cases ***\n",
            "TC_45 Validate Merchant Can View Order History\n",
            "[Documentation]    Validate TC_45 Validate Merchant Can View Order History using the following steps\n",
            "Select Module    ${Orders}    ${}    ${}\n",
            "Navigate To Order History Page    ${month}    ${}    ${}\n",
            "Validate Order Entries Are Displayed    ${order_id}    ${}    ${}\n",
            "Filter Orders By Status    ${Completed}    ${}    ${}\n",
            "Validate Filtered Orders    ${order_id}    ${Completed}    ${}\n",
            "\n",
            "*** Keywords ***\n",
            "Select Module\n",
            "[Arguments]    ${Orders}    ${}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${Orders}    ${}    ${}\n",
            "\n",
            "Navigate To Order History Page\n",
            "[Arguments]    ${month}    ${}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${month}    ${}    ${}\n",
            "\n",
            "Validate Order Entries Are Displayed\n",
            "[Arguments]    ${order_id}    ${}    ${}\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${order_id}    ${}    ${}\n",
            "\n",
            "Filter Orders By Status\n",
            "[Arguments]    ${Completed}    ${}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${Completed}    ${}    ${}\n",
            "\n",
            "Validate Filtered Orders\n",
            "[Arguments]    ${order_id}    ${Completed}    ${}\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${order_id}    ${Completed}    ${}\n",
            "<|end_of_text|>\n",
            "######################################################################################\n",
            "AI RESPONSE: \n",
            "[   '<|begin_of_text|>Below is an instruction that describes a task, paired '\n",
            "    'with an input that provides further context. Write a response that '\n",
            "    'appropriately completes the request.\\n'\n",
            "    '\\n'\n",
            "    '### Instruction:\\n'\n",
            "    'Generate data-driven clientcentral_testcases for orders Testcases TC_45 '\n",
            "    'Validate Merchant Can View Order History.\\n'\n",
            "    '\\n'\n",
            "    '### Input:\\n'\n",
            "    'Test Case Type: data-driven  \\n'\n",
            "    ' Test Case Name: TC_45 Validate Merchant Can View Order History\\n'\n",
            "    '\\n'\n",
            "    '### Response:\\n'\n",
            "    '*** Settings ***\\n'\n",
            "    'Documentation    Validate TC_45 Validate Merchant Can View Order History\\n'\n",
            "    'Test Template    Test Execution\\n'\n",
            "    'Resource         resource_file.robot\\n'\n",
            "    '\\n'\n",
            "    '*** Variables ***\\n'\n",
            "    '${VALID_USER}       testuser\\n'\n",
            "    '${VALID_PASSWORD}   secret\\n'\n",
            "    '\\n'\n",
            "    '*** Test Cases ***\\n'\n",
            "    'TC_45 Validate Merchant Can View Order History\\n'\n",
            "    '[Documentation]    Validate TC_45 Validate Merchant Can View Order '\n",
            "    'History using the following steps\\n'\n",
            "    'Login As Merchant    ${VALID_USER}    ${VALID_PASSWORD}    ${}\\n'\n",
            "    'Navigate To Order History Page    ${}    ${}    ${}\\n'\n",
            "    'Validate Order History Page Is Displayed    ${}    ${}    ${}\\n'\n",
            "    'Verify Orders In History    ${}    ${}    ${}\\n'\n",
            "    'Filter Orders By Date Range    ${start_date}    ${end_date}    ${}\\n'\n",
            "    'Validate Filtered Orders Are Displayed    ${}    ${}    ${}\\n'\n",
            "    'Download Order History Report    ${}    ${}    ${}\\n'\n",
            "    'Validate Report Is Downloaded    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    '*** Keywords ***\\n'\n",
            "    'Login As Merchant\\n'\n",
            "    '[Arguments]    ${VALID_USER}    ${VALID_PASSWORD}    ${}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${VALID_USER}    ${VALID_PASSWORD}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Navigate To Order History Page\\n'\n",
            "    '[Arguments]    ${}    ${}    ${}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Validate Order History Page Is Displayed\\n'\n",
            "    '[Arguments]    ${}    ${}    ${}\\n'\n",
            "    '[Return]    ${boolean}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Verify Orders In History\\n'\n",
            "    '[Arguments]    ${}    ${}    ${}\\n'\n",
            "    '[Return]    ${boolean}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Filter Orders By Date Range\\n'\n",
            "    '[Arguments]    ${start_date}    ${end_date}    ${}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${start_date}    ${end_date}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Validate Filtered Orders Are Displayed\\n'\n",
            "    '[Arguments]    ${}    ${}    ${}\\n'\n",
            "    '[Return]    ${boolean}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Download Order History Report\\n'\n",
            "    '[Arguments]    ${}    ${}    ${}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    'Validate Report Is Downloaded\\n'\n",
            "    '[Arguments]    ${}    ${}    ${}\\n'\n",
            "    '[Return]    ${boolean}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${}    ${}    ${}\\n'\n",
            "    '\\n'\n",
            "    '<|end_of_text|>']\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "print(\"TEST EXAMPLE: \", dataset['test']['text'][5])\n",
        "\n",
        "print(\"######################################################################################\")\n",
        "\n",
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Generate data-driven clientcentral_testcases for orders Testcases TC_45 Validate Merchant Can View Order History.\", # instruction\n",
        "        \"Test Case Type: data-driven  \\n Test Case Name: TC_45 Validate Merchant Can View Order History\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 600, use_cache = True)\n",
        "print(\"AI RESPONSE: \")\n",
        "pp.pprint(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmC8q1mOqGif",
        "outputId": "63250f5c-0590-4e43-a0ac-1176b0044ed9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST EXAMPLE:  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Validate data-driven paynow test cases for Sterlingcooper Testcases TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card\n",
            "\n",
            "### Input:\n",
            "Test Case Type: data-driven\n",
            " Test Case Name: TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card\n",
            "\n",
            "### Response:\n",
            "*** Settings ***\n",
            "Documentation    Validate TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card\n",
            "Test Template    Test Execution\n",
            "Resource         resource_file.robot\n",
            "\n",
            "*** Variables ***\n",
            "${VALID_USER}       testuser\n",
            "${VALID_PASSWORD}   secret\n",
            "\n",
            "*** Test Cases ***\n",
            "TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card\n",
            "[Documentation]    Validate TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card using the following steps\n",
            "Generate Random Customer Details    ${nan}    ${nan}    ${}\n",
            "Launch PayNow Application    ${STERLING_COOPER}    ${nan}    ${}\n",
            "Enter Account Number And Invoice    ${customer_details}    ${invoice_number}    ${}\n",
            "Select Payment Method    ${Credit Card}    ${expired_card_details}    ${}\n",
            "Attempt To Process Payment    ${expired_card_details}    ${nan}    ${}\n",
            "Validate Error Message For Expired Card    ${\"Your credit card has expired\"}    ${nan}    ${}\n",
            "\n",
            "*** Keywords ***\n",
            "Generate Random Customer Details\n",
            "[Arguments]    ${nan}    ${nan}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${nan}    ${nan}    ${}\n",
            "\n",
            "Launch PayNow Application\n",
            "[Arguments]    ${STERLING_COOPER}    ${nan}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${STERLING_COOPER}    ${nan}    ${}\n",
            "\n",
            "Enter Account Number And Invoice\n",
            "[Arguments]    ${customer_details}    ${invoice_number}    ${}\n",
            "[Return]    ${integer}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${customer_details}    ${invoice_number}    ${}\n",
            "\n",
            "Select Payment Method\n",
            "[Arguments]    ${Credit Card}    ${expired_card_details}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${Credit Card}    ${expired_card_details}    ${}\n",
            "\n",
            "Attempt To Process Payment\n",
            "[Arguments]    ${expired_card_details}    ${nan}    ${}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${expired_card_details}    ${nan}    ${}\n",
            "\n",
            "Validate Error Message For Expired Card\n",
            "[Arguments]    ${\"Your credit card has expired\"}    ${nan}    ${}\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${\"Your credit card has expired\"}    ${nan}    ${}\n",
            "\n",
            "<|end_of_text|>\n",
            "######################################################################################\n",
            "AI RESPONSE: \n",
            "[   '<|begin_of_text|>Below is an instruction that describes a task, paired '\n",
            "    'with an input that provides further context. Write a response that '\n",
            "    'appropriately completes the request.\\n'\n",
            "    '\\n'\n",
            "    '### Instruction:\\n'\n",
            "    'Validate data-driven paynow test cases for Sterlingcooper Testcases '\n",
            "    'TC_124 Validate Specific Error Message When Making Payment With Expired '\n",
            "    'Credit Card.\\n'\n",
            "    '\\n'\n",
            "    '### Input:\\n'\n",
            "    'Test Case Type: data-driven  \\n'\n",
            "    'Test Case Name: TC_124 Validate Specific Error Message When Making '\n",
            "    'Payment With Expired Credit Card\\n'\n",
            "    '\\n'\n",
            "    '### Response:\\n'\n",
            "    '*** Settings ***\\n'\n",
            "    'Documentation    Validate TC_124 Validate Specific Error Message When '\n",
            "    'Making Payment With Expired Credit Card\\n'\n",
            "    'Test Template    Test Execution\\n'\n",
            "    'Resource         resource_file.robot\\n'\n",
            "    '\\n'\n",
            "    '*** Variables ***\\n'\n",
            "    '${VALID_USER}       testuser\\n'\n",
            "    '${VALID_PASSWORD}   secret\\n'\n",
            "    '\\n'\n",
            "    '*** Test Cases ***\\n'\n",
            "    'TC_124 Validate Specific Error Message When Making Payment With Expired '\n",
            "    'Credit Card\\n'\n",
            "    '[Documentation]    Validate TC_124 Validate Specific Error Message When '\n",
            "    'Making Payment With Expired Credit Card using the following steps\\n'\n",
            "    'Send Request To Create Customer    ${STERLING_COOPER_MERCHANT}    '\n",
            "    '${nan}    ${nan}\\n'\n",
            "    'Send Request To Create Invoice For Customer    ${account_number}    '\n",
            "    '${STERLING_COOPER_MERCHANT}    ${nan}\\n'\n",
            "    'Launch PayNow Application    ${STERLING_COOPER_MERCHANT}    ${nan}    '\n",
            "    '${nan}\\n'\n",
            "    'Enter Account Number And Invoice    ${account_number}    '\n",
            "    '${invoice_number}    ${nan}\\n'\n",
            "    'Select Payment Method    ${Credit Card}    ${nan}    ${nan}\\n'\n",
            "    'Enter Credit Card Details    ${expired_card_details}    ${nan}    ${nan}\\n'\n",
            "    'Validate Error Message For Expired Card    ${expired_card_message}    '\n",
            "    '${nan}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    '*** Keywords ***\\n'\n",
            "    'Send Request To Create Customer\\n'\n",
            "    '[Arguments]    ${STERLING_COOPER_MERCHANT}    ${nan}    ${nan}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${STERLING_COOPER_MERCHANT}    ${nan}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    'Send Request To Create Invoice For Customer\\n'\n",
            "    '[Arguments]    ${account_number}    ${STERLING_COOPER_MERCHANT}    '\n",
            "    '${nan}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${account_number}    ${STERLING_COOPER_MERCHANT}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    'Launch PayNow Application\\n'\n",
            "    '[Arguments]    ${STERLING_COOPER_MERCHANT}    ${nan}    ${nan}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${STERLING_COOPER_MERCHANT}    ${nan}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    'Enter Account Number And Invoice\\n'\n",
            "    '[Arguments]    ${account_number}    ${invoice_number}    ${nan}\\n'\n",
            "    '[Return]    ${integer}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${account_number}    ${invoice_number}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    'Select Payment Method\\n'\n",
            "    '[Arguments]    ${Credit Card}    ${nan}    ${nan}\\n'\n",
            "    '[Return]    ${string}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${Credit Card}    ${nan}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    'Enter Credit Card Details\\n'\n",
            "    '[Arguments]    ${expired_card_details}    ${nan}    ${nan}\\n'\n",
            "    '[Return]    ${integer}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${expired_card_details}    ${nan}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    'Validate Error Message For Expired Card\\n'\n",
            "    '[Arguments]    ${expired_card_message}    ${nan}    ${nan}\\n'\n",
            "    '[Return]    ${boolean}\\n'\n",
            "    '[Documentation]    Keyword description\\n'\n",
            "    'Log    ${expired_card_message}    ${nan}    ${nan}\\n'\n",
            "    '\\n'\n",
            "    '<|end_of_text|>']\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "print(\"TEST EXAMPLE: \", dataset['test']['text'][20])\n",
        "\n",
        "print(\"######################################################################################\")\n",
        "\n",
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Validate data-driven paynow test cases for Sterlingcooper Testcases TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card.\", # instruction\n",
        "        \"Test Case Type: data-driven  \\nTest Case Name: TC_124 Validate Specific Error Message When Making Payment With Expired Credit Card\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 700, use_cache = True)\n",
        "print(\"AI RESPONSE: \")\n",
        "pp.pprint(tokenizer.batch_decode(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDJk_q9jqmGn",
        "outputId": "83d7c0e8-a5b8-474c-8134-d7cf0135c7a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TEST EXAMPLE:  Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create keyword-driven test case for MyApp user_authentication\n",
            "\n",
            "### Input:\n",
            "Test Case Type: keyword-driven\n",
            " Test Case Name: user_authentication\n",
            "\n",
            "### Response:\n",
            "*** Settings ***\n",
            "Documentation    Validate user_authentication\n",
            "Test Setup       Initialize Environment\n",
            "Test Teardown    Cleanup Environment\n",
            "Suite Setup      Open Application\n",
            "Suite Teardown   Close Application\n",
            "Resource         resource_file.robot\n",
            "\n",
            "*** Variables ***\n",
            "${VALID_USER}       testuser\n",
            "${VALID_PASSWORD}   secret\n",
            "\n",
            "*** Test Cases ***\n",
            "user_authentication\n",
            "[Documentation]    Validate user_authentication using the following steps\n",
            "Enter Username And Password    username|password\n",
            "Click Login Button    nan\n",
            "Validate Login Success    expected_message\n",
            "Logout User    nan\n",
            "Enter Invalid Username    invalid_username\n",
            "Enter Invalid Password    invalid_password\n",
            "Click Login Button    nan\n",
            "Validate Authentication Failure    expected_error\n",
            "Reset Password    username|new_password\n",
            "Validate Password Reset Success    expected_message\n",
            "Enter New Username And Password    new_username|new_password\n",
            "Click Login Button    nan\n",
            "Validate Login Success    expected_message\n",
            "\n",
            "*** Keywords ***\n",
            "Enter Username And Password\n",
            "[Arguments]    username|password\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    username|password\n",
            "\n",
            "Click Login Button\n",
            "[Arguments]    nan\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    nan\n",
            "\n",
            "Validate Login Success\n",
            "[Arguments]    expected_message\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    expected_message\n",
            "\n",
            "Logout User\n",
            "[Arguments]    nan\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    nan\n",
            "\n",
            "Enter Invalid Username\n",
            "[Arguments]    invalid_username\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    invalid_username\n",
            "\n",
            "Enter Invalid Password\n",
            "[Arguments]    invalid_password\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    invalid_password\n",
            "\n",
            "Validate Authentication Failure\n",
            "[Arguments]    expected_error\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    expected_error\n",
            "\n",
            "Reset Password\n",
            "[Arguments]    username|new_password\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    username|new_password\n",
            "\n",
            "Validate Password Reset Success\n",
            "[Arguments]    expected_message\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    expected_message\n",
            "\n",
            "Enter New Username And Password\n",
            "[Arguments]    new_username|new_password\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    new_username|new_password\n",
            "\n",
            "<|end_of_text|>\n",
            "######################################################################################\n",
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Create keyword-driven test case for MyApp user_authentication.\n",
            "\n",
            "### Input:\n",
            "Test Case Type: keyword-driven  \n",
            "Test Case Name: user_authentication\n",
            "\n",
            "### Response:\n",
            "*** Settings ***\n",
            "Documentation    Validate user_authentication\n",
            "Test Setup       Initialize Environment\n",
            "Test Teardown    Cleanup Environment\n",
            "Suite Setup      Open Application\n",
            "Suite Teardown   Close Application\n",
            "Resource         resource_file.robot\n",
            "\n",
            "*** Variables ***\n",
            "${VALID_USER}       testuser\n",
            "${VALID_PASSWORD}   secret\n",
            "\n",
            "*** Test Cases ***\n",
            "user_authentication\n",
            "[Documentation]    Validate user_authentication using the following steps\n",
            "Login With Valid Credentials    ${VALID_USER}    ${VALID_PASSWORD}\n",
            "Validate Login Success Message    success_message\n",
            "Logout\n",
            "Login With Invalid Credentials    invalid_user    invalid_password\n",
            "Validate Login Failure Message    failure_message\n",
            "Logout\n",
            "Login With Empty Credentials    empty_user    empty_password\n",
            "Validate Error Message For Empty Credentials    error_message\n",
            "Logout\n",
            "\n",
            "*** Keywords ***\n",
            "Login With Valid Credentials\n",
            "[Arguments]    user, password\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    user, password\n",
            "\n",
            "Validate Login Success Message\n",
            "[Arguments]    success_message\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    success_message\n",
            "\n",
            "Logout\n",
            "[Arguments]    ${boolean}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${boolean}\n",
            "\n",
            "Login With Invalid Credentials\n",
            "[Arguments]    invalid_user, invalid_password\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    invalid_user, invalid_password\n",
            "\n",
            "Validate Login Failure Message\n",
            "[Arguments]    failure_message\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    failure_message\n",
            "\n",
            "Logout\n",
            "[Arguments]    ${boolean}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${boolean}\n",
            "\n",
            "Login With Empty Credentials\n",
            "[Arguments]    empty_user, empty_password\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    empty_user, empty_password\n",
            "\n",
            "Validate Error Message For Empty Credentials\n",
            "[Arguments]    error_message\n",
            "[Return]    ${boolean}\n",
            "[Documentation]    Keyword description\n",
            "Log    error_message\n",
            "\n",
            "Logout\n",
            "[Arguments]    ${boolean}\n",
            "[Return]    ${string}\n",
            "[Documentation]    Keyword description\n",
            "Log    ${boolean}\n",
            "\n",
            "<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "print(\"TEST EXAMPLE: \", dataset['test']['text'][90])\n",
        "\n",
        "print(\"######################################################################################\")\n",
        "\n",
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Create keyword-driven test case for MyApp user_authentication.\", # instruction\n",
        "        \"Test Case Type: keyword-driven  \\nTest Case Name: user_authentication\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 700)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "80b3f0f7-7b8f-4576-b428-4708fc43bdc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "# if False:\n",
        "#     from unsloth import FastLanguageModel\n",
        "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "#         max_seq_length = max_seq_length,\n",
        "#         dtype = dtype,\n",
        "#         load_in_4bit = load_in_4bit,\n",
        "#     )\n",
        "#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# # alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     alpaca_prompt.format(\n",
        "#         \"What is a famous tall tower in Paris?\", # instruction\n",
        "#         \"\", # input\n",
        "#         \"\", # output - leave this blank for generation!\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "# tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHUiSGVdzvtd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"robotFramwork-sta-llama3-8b-testcase-model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/robotFramwork-sta-llama3-8b-testcase-model\", tokenizer, save_method = \"merged_16bit\", token = \"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\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"robotFramwork-sta-llama3-8b-testcase-model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"chukypedro/robotFramwork-sta-llama3-8b-testcase-model\", tokenizer, save_method = \"merged_4bit\", token = \"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\")\n",
        "\n",
        "# # Just LoRA adapters\n",
        "# if True: model.save_pretrained_merged(\"robotFramwork-sta-llama3-8b-testcase-model\", tokenizer, save_method = \"lora\",)\n",
        "# if True: model.push_to_hub_merged(\"chukypedro/robotFramwork-sta-llama3-8b-testcase-model\", tokenizer, save_method = \"lora\", token = \"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8af99b5146f547d1bfe1c8a1a7301b93",
            "c929d862e4b646e4819919634137bd83",
            "fcecdb17b87d48819c948ef33fca50ff",
            "94fb61ea675f4035bd9488adc92a6ae7",
            "0aa621ebd40142f5888ab0f27f668ac8",
            "ca2950f053074ff69edb5d99d1e8e0c3",
            "58e7a7401cb24059b383728abd596cb4",
            "e1011d0c7f6149239629972063e633e7",
            "c0cd6a2b57e64e7a926b97ab832a3eca",
            "c005a57235c6497196e59f3e3c14a5c6",
            "581723738c45466da093b130a55a7b4a",
            "0ec1b67f7e514b449f64df7675ba6782",
            "c717025c548f439f9deab7ba52905a94",
            "a2b093af03ae4c66bc628f51927d8160",
            "76bd31859d314150aa25bca76aae7919",
            "d2ac1611ba824460b954486d9022c5d1",
            "83c80cf8d2494f8caaaa63f1d2d40eae",
            "b1d27a633428425587f402cbedd3cdb2",
            "f3e5b37fd30d404a91a863eea58be66c",
            "49265549d3314062a09fbd5fd0e0922f",
            "77623fb39e1141238afa26f6e761a911",
            "a034ee440b764707ae218523b4c37a51",
            "e88b7aafb0d347c19be8efafbd339006",
            "f318178c593546e88d3235aaf666e299",
            "08d5836446094abead951cb0332e21d9",
            "0543c7b7b5544fc58e4eb46d67da51b8",
            "0c8291775bbf4885b6db7a5e5fa4a7b5",
            "ea21abff121f41d6b4e6ed95eedfa4fe",
            "73bf7a881b68469eb13472b59efc4fba",
            "2647423bd8134ad6b1a96fdc045e51a1",
            "f35e84c97371452e94616449b1b6e215",
            "bf7e337307f343c49c68d0b53aefdd63",
            "6b70504d09694c1a9130a0d443ab2e22",
            "4ecb0fb912ac415f84367fb6d083c62b",
            "a69d315121c24d4d8df58666638e88c5",
            "301af946b1734c0b9cedea8ed7d06049",
            "23f5b0438bdc436690e7d29df861e80c",
            "d958d6df5aec4d8eb86c07c78d21948a",
            "17e7fa0037f348feab4db1f2442e6225",
            "2fd5c7c1e3324124b7ffcda58bd43679",
            "45a9f49d2dba4c6d983b2f09d8444cd4",
            "03df5e9256b446508af621e215756507",
            "6ad7cd49269f418eb21ed51e37b7f5f5",
            "d40c76d6b420412ea41b2e62604cb416"
          ]
        },
        "id": "Jxb_s9dBz6gV",
        "outputId": "1bf536c4-851e-467d-c4e4-397d112d3a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 59.71 out of 83.48 RAM for saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:00<00:00, 65.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
            "Done.\n",
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q8_0', 'q5_k_m'] will take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
            "Unsloth: [1] Converting model at chukypedro/model into bf16 GGUF format.\n",
            "The output location will be ./chukypedro/model/unsloth.BF16.gguf\n",
            "This will take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128001\n",
            "INFO:gguf.vocab:Setting special token type pad to 128255\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:chukypedro/model/unsloth.BF16.gguf: n_tensors = 291, total_size = 16.1G\n",
            "Writing: 100%|██████████| 16.1G/16.1G [01:58<00:00, 136Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to chukypedro/model/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: ./chukypedro/model/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
            "main: build = 3345 (2ee44c9a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './chukypedro/model/unsloth.BF16.gguf' to './chukypedro/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./chukypedro/model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = model\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q4_K .. size =  1002.00 MiB ->   281.81 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n",
            "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 15317.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4685.30 MB\n",
            "\n",
            "main: quantize time = 145653.94 ms\n",
            "main:    total time = 145653.94 ms\n",
            "Unsloth: Conversion completed! Output location: ./chukypedro/model/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q8_0. This will take 20 minutes...\n",
            "main: build = 3345 (2ee44c9a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './chukypedro/model/unsloth.BF16.gguf' to './chukypedro/model/unsloth.Q8_0.gguf' as Q8_0 using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./chukypedro/model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = model\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q8_0 .. size =  1002.00 MiB ->   532.31 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
            "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q8_0 .. size =  1002.00 MiB ->   532.31 MiB\n",
            "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 15317.02 MB\n",
            "llama_model_quantize_internal: quant size  =  8137.64 MB\n",
            "\n",
            "main: quantize time = 25188.55 ms\n",
            "main:    total time = 25188.55 ms\n",
            "Unsloth: Conversion completed! Output location: ./chukypedro/model/unsloth.Q8_0.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This will take 20 minutes...\n",
            "main: build = 3345 (2ee44c9a)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing './chukypedro/model/unsloth.BF16.gguf' to './chukypedro/model/unsloth.Q5_K_M.gguf' as Q5_K_M using 24 threads\n",
            "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ./chukypedro/model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = model\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128255\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type bf16:  226 tensors\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q5_K .. size =  1002.00 MiB ->   344.44 MiB\n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =   bf16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
            "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
            "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =   bf16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
            "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =   bf16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
            "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =   bf16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
            "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =   bf16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
            "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 15317.02 MB\n",
            "llama_model_quantize_internal: quant size  =  5459.93 MB\n",
            "\n",
            "main: quantize time = 118233.71 ms\n",
            "main:    total time = 118233.71 ms\n",
            "Unsloth: Conversion completed! Output location: ./chukypedro/model/unsloth.Q5_K_M.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8af99b5146f547d1bfe1c8a1a7301b93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.BF16.gguf:   0%|          | 0.00/16.1G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/chukypedro/model\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ec1b67f7e514b449f64df7675ba6782",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/chukypedro/model\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e88b7aafb0d347c19be8efafbd339006",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/chukypedro/model\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ecb0fb912ac415f84367fb6d083c62b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q5_K_M.gguf:   0%|          | 0.00/5.73G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/chukypedro/model\n"
          ]
        }
      ],
      "source": [
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if True:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"chukypedro/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"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\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# # Save to 8bit Q8_0\n",
        "# if False: model.save_pretrained_gguf(\"robotFramwork-sta-llama3-8b_gguf\", tokenizer,)\n",
        "# if True: model.push_to_hub_gguf(\"chukypedro/robotFramwork-sta-llama3-8b_gguf\", tokenizer, token = \"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\")\n",
        "# # Save to 16bit GGUF\n",
        "# # if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "# # if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"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\")\n",
        "\n",
        "# # Save to q4_k_m GGUF\n",
        "# if True: model.save_pretrained_gguf(\"robotFramwork-sta-llama3-8b_gguf\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "# if True: model.push_to_hub_gguf(\"chukypedro/robotFramwork-sta-llama3-8b_gguf\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\")\n",
        "\n",
        "# # Save to q5_k_m GGUF\n",
        "# if True: model.save_pretrained_gguf(\"robotFramwork-sta-llama3-8b_gguf\", tokenizer, quantization_method = \"q5_k_m\")\n",
        "# if True: model.push_to_hub_gguf(\"chukypedro/robotFramwork-sta-llama3-8b_gguf\", tokenizer, quantization_method = \"q5_k_m\", token = \"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with 🤗 HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00ecc999da0d44a6ac1094adabd31c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01ce1b9cf5ac4e4fadcfa849d87032dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03c196304147433ebb5f3c58550059f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03df5e9256b446508af621e215756507": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04ed7f3609e44cf1a1ac1e20037a520f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0543c7b7b5544fc58e4eb46d67da51b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf7e337307f343c49c68d0b53aefdd63",
            "placeholder": "​",
            "style": "IPY_MODEL_6b70504d09694c1a9130a0d443ab2e22",
            "value": " 8.54G/8.54G [05:06&lt;00:00, 24.8MB/s]"
          }
        },
        "06b026e54c4f47d8b83aafbec2247d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c05e64f00c4b7ba2524b3d791f9a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08d5836446094abead951cb0332e21d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2647423bd8134ad6b1a96fdc045e51a1",
            "max": 8540770176,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f35e84c97371452e94616449b1b6e215",
            "value": 8540770176
          }
        },
        "09044b4f54054eedb09bc469b8a90cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_509aee3ff9f748228091ee94dd340a52",
              "IPY_MODEL_f060ffe3b4e94f46ab6a32b39caf3b82",
              "IPY_MODEL_97b54e6d916742ed9648b485858ad21f"
            ],
            "layout": "IPY_MODEL_413309d100c7448591ee438f77d9e705"
          }
        },
        "0964570818894eda983113ef33671291": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c6448b18d143ad91990a74f95510cb",
            "placeholder": "​",
            "style": "IPY_MODEL_2cd3bd2e23564d86ae7110458d064146",
            "value": "Generating train split: 100%"
          }
        },
        "0a79ff85146f437aa1b825e39c772135": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aa621ebd40142f5888ab0f27f668ac8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8291775bbf4885b6db7a5e5fa4a7b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ec1b67f7e514b449f64df7675ba6782": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c717025c548f439f9deab7ba52905a94",
              "IPY_MODEL_a2b093af03ae4c66bc628f51927d8160",
              "IPY_MODEL_76bd31859d314150aa25bca76aae7919"
            ],
            "layout": "IPY_MODEL_d2ac1611ba824460b954486d9022c5d1"
          }
        },
        "114b6248ab434e6097894a66f54cae71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14c6448b18d143ad91990a74f95510cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14f4435713044a9ba9ec38e1a588ef6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e7fa0037f348feab4db1f2442e6225": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "194f8890bf234c78970e31a5f4c455ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "195ac429386240e49f9e56a601f61cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8348574c0088456e878d70f3389b3f45",
            "max": 464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db349839b0e14b03aca8fce24b40eb72",
            "value": 464
          }
        },
        "1ac3231910f445328d478b2ad358c79b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd98275af5040a5b8bed7651def4fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c72734232014f8281fe5c6489dbdec7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fbb4b9e325241009972bbaa4a788c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21d1a908a479406ca1ac163ded0bb487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d955f14d407f4d3686ab7dd1bcdc7ed7",
              "IPY_MODEL_6cf1a8b6d8754363aca6cd880ce79188",
              "IPY_MODEL_6581633425ff46bfbc76424e91f2bad3"
            ],
            "layout": "IPY_MODEL_8954dafd688e47ab82e4c1f94ab68e6c"
          }
        },
        "231979b0d03c471fb763e737eca12093": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d837e42e687f479a94bebfd10dcd42c5",
            "max": 172,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e43752904c8240a5a86f8744ee4ce025",
            "value": 172
          }
        },
        "23201ef69b6d42099ab722ade8f2e852": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_486d45c7168b414eaec85426be564f7c",
              "IPY_MODEL_231979b0d03c471fb763e737eca12093",
              "IPY_MODEL_d916993dcf6347379221a523eae5104c"
            ],
            "layout": "IPY_MODEL_d4fb4d6f05314372a41431013f7d0383"
          }
        },
        "2349e817f085499788cb7c3e41ef4844": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fb476d1c2a643c483687208175ee456",
            "max": 50641,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53a189a985204b9ea737eacdd9bd4288",
            "value": 50641
          }
        },
        "23f5b0438bdc436690e7d29df861e80c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad7cd49269f418eb21ed51e37b7f5f5",
            "placeholder": "​",
            "style": "IPY_MODEL_d40c76d6b420412ea41b2e62604cb416",
            "value": " 5.73G/5.73G [03:41&lt;00:00, 36.9MB/s]"
          }
        },
        "2466e42ae8cf431a9fd8606c2a62e2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_337069e1a13849db8073796f372781cd",
              "IPY_MODEL_d8ae6010d2954d58bbe4b1de7998322d",
              "IPY_MODEL_53d52987cf77436fad96e74a9b22f4bb"
            ],
            "layout": "IPY_MODEL_03c196304147433ebb5f3c58550059f3"
          }
        },
        "24a2aff08a0142a3b91c0be3fc1bb4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2647423bd8134ad6b1a96fdc045e51a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28230f6db2244ef5a457b520429f83d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c04c96a30d74f6aa67c8ac1092d9eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c402ad9763645adb687f4a57e828e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06b026e54c4f47d8b83aafbec2247d1c",
            "placeholder": "​",
            "style": "IPY_MODEL_775160414acd471589da1667db3499e7",
            "value": "Map: 100%"
          }
        },
        "2cd3bd2e23564d86ae7110458d064146": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f81d60fa29e4280bbf5315c69151d30": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd5c7c1e3324124b7ffcda58bd43679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "301af946b1734c0b9cedea8ed7d06049": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45a9f49d2dba4c6d983b2f09d8444cd4",
            "max": 5732986752,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03df5e9256b446508af621e215756507",
            "value": 5732986752
          }
        },
        "30aadb310e6047d6a89a27231c54c317": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8a64aff44ed4d97821e168014c39226",
              "IPY_MODEL_32b58b96f6b34ea1a8e71684fa6f6de5",
              "IPY_MODEL_fb171cd945294e11ab7746f4f188b580"
            ],
            "layout": "IPY_MODEL_9d04661880b547e6a37ad1ad3c31e9f4"
          }
        },
        "31be97e54c2e45368f0798a5fcb86e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b58b96f6b34ea1a8e71684fa6f6de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f37ecb851f42fe85a3a84709e78e80",
            "max": 4563,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_114b6248ab434e6097894a66f54cae71",
            "value": 4563
          }
        },
        "337069e1a13849db8073796f372781cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af4083475674845a1daea699508cd7f",
            "placeholder": "​",
            "style": "IPY_MODEL_df9f4b1ce9984ac5b45157039873299e",
            "value": "Downloading data: 100%"
          }
        },
        "3500406046e6400daefebd21cf9d15ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36624764402e4fe1adcc8656c8e25038": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3666db114441410fb66363d64b6ea24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0964570818894eda983113ef33671291",
              "IPY_MODEL_8ac28dac0a1241dcabedf19bf0101d1d",
              "IPY_MODEL_3b443b6c1ec14824a6aeec2e195fb2c0"
            ],
            "layout": "IPY_MODEL_b3ba41c4fa184c9e8ab3f33f4e01ff88"
          }
        },
        "384b85c6341747cb8b5b4d71362dd0f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389481a1e6e744909196bb4642bfb91a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b443b6c1ec14824a6aeec2e195fb2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ac3231910f445328d478b2ad358c79b",
            "placeholder": "​",
            "style": "IPY_MODEL_7c48676eaa1246068486a6f1326ccc3a",
            "value": " 1069/1069 [00:00&lt;00:00, 17182.46 examples/s]"
          }
        },
        "3c810fd75e4946b483d9e7ffbbfd0d78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d6c58aea24649ec9deb416e93731bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31be97e54c2e45368f0798a5fcb86e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_01ce1b9cf5ac4e4fadcfa849d87032dc",
            "value": " 962/962 [00:01&lt;00:00, 662.68 examples/s]"
          }
        },
        "3e0abf68ef1c424291268a30155cb6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97037f8b83d14d848fd34cabb064bbfc",
              "IPY_MODEL_808dc110997e4f83937edc38a143e69a",
              "IPY_MODEL_df03e34db2f54119b307180123765b71"
            ],
            "layout": "IPY_MODEL_36624764402e4fe1adcc8656c8e25038"
          }
        },
        "413309d100c7448591ee438f77d9e705": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c84c1537984cfca9c5c48c0a078c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_194f8890bf234c78970e31a5f4c455ba",
            "placeholder": "​",
            "style": "IPY_MODEL_389481a1e6e744909196bb4642bfb91a",
            "value": " 9.09M/9.09M [00:02&lt;00:00, 3.56MB/s]"
          }
        },
        "450388b09b4340ffb4228366eedb9669": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45a9f49d2dba4c6d983b2f09d8444cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ac9f3fc0784d41b67afa5d1386835a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "470796be52464966b9fc98cc6fc88d25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477c22f48991468da5d23f5253cdaa6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d4e610999e4e16b8564fc483c9a4d6",
            "max": 1069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_835045ed12b44e53909ba8453aa7f624",
            "value": 1069
          }
        },
        "486bd0ff79514cff913afab0e37049a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "486d45c7168b414eaec85426be564f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80af76274b454071953adf69b0286079",
            "placeholder": "​",
            "style": "IPY_MODEL_bfa39246b2be4370b911a6ee5e72e118",
            "value": "generation_config.json: 100%"
          }
        },
        "49265549d3314062a09fbd5fd0e0922f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a06cf3bf52e490c8ff78a5ed0b5af8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b8e1d8a715f4d2b8ab0734f4f90a00e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eba10188f2e4481b7cb77b7a6303684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_470796be52464966b9fc98cc6fc88d25",
            "placeholder": "​",
            "style": "IPY_MODEL_1bd98275af5040a5b8bed7651def4fa5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4ecb0fb912ac415f84367fb6d083c62b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a69d315121c24d4d8df58666638e88c5",
              "IPY_MODEL_301af946b1734c0b9cedea8ed7d06049",
              "IPY_MODEL_23f5b0438bdc436690e7d29df861e80c"
            ],
            "layout": "IPY_MODEL_d958d6df5aec4d8eb86c07c78d21948a"
          }
        },
        "4fb476d1c2a643c483687208175ee456": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "509aee3ff9f748228091ee94dd340a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2983ff29804c82b2a2c33673030bda",
            "placeholder": "​",
            "style": "IPY_MODEL_04ed7f3609e44cf1a1ac1e20037a520f",
            "value": "config.json: 100%"
          }
        },
        "530f7170009a47129ff384a7d6ef1034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450388b09b4340ffb4228366eedb9669",
            "placeholder": "​",
            "style": "IPY_MODEL_1fbb4b9e325241009972bbaa4a788c92",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "53a189a985204b9ea737eacdd9bd4288": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53d52987cf77436fad96e74a9b22f4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffc851c5002e4690ad3b7362e776b371",
            "placeholder": "​",
            "style": "IPY_MODEL_8f954e42c4074441a3ec1d0f9393c995",
            "value": " 3.03M/3.03M [00:00&lt;00:00, 3.45MB/s]"
          }
        },
        "57c19b5efd0a4678b0fca61aa3fd84b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581723738c45466da093b130a55a7b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583b91cac2444692b41f2c515d592345": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e7a7401cb24059b383728abd596cb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f6255e2fae4f589c2c8dcc49cf2468": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af4083475674845a1daea699508cd7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ea2823510574579b28aaf17250c6a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7530a2f26b64f3489fd10bf15bc2139",
            "max": 962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcc224472ca241098a7eee0bb93c0f10",
            "value": 962
          }
        },
        "5f6e6f69ba44454e8f891e885611b3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6581633425ff46bfbc76424e91f2bad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3500406046e6400daefebd21cf9d15ae",
            "placeholder": "​",
            "style": "IPY_MODEL_45ac9f3fc0784d41b67afa5d1386835a",
            "value": " 107/107 [00:01&lt;00:00, 127.70 examples/s]"
          }
        },
        "6ad7cd49269f418eb21ed51e37b7f5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2983ff29804c82b2a2c33673030bda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b70504d09694c1a9130a0d443ab2e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cf1a8b6d8754363aca6cd880ce79188": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a6d9d60705412abb332dd8d14e506f",
            "max": 107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac14d1761ed847b8a414e456a66ab09e",
            "value": 107
          }
        },
        "72c19ba142334e31890e0959412f0432": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d4e610999e4e16b8564fc483c9a4d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73bf7a881b68469eb13472b59efc4fba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76bd31859d314150aa25bca76aae7919": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77623fb39e1141238afa26f6e761a911",
            "placeholder": "​",
            "style": "IPY_MODEL_a034ee440b764707ae218523b4c37a51",
            "value": " 4.92G/4.92G [02:51&lt;00:00, 28.7MB/s]"
          }
        },
        "775160414acd471589da1667db3499e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77623fb39e1141238afa26f6e761a911": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d1883404394feca2a1b746a1179d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79d9e31ccc384903bbf1b8b8bc3af583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b497abd5a164163928b1e83ebed2951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c279da6d42a4de789e8e47c67e9bd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c810fd75e4946b483d9e7ffbbfd0d78",
            "placeholder": "​",
            "style": "IPY_MODEL_2c04c96a30d74f6aa67c8ac1092d9eca",
            "value": " 1069/1069 [00:00&lt;00:00, 28470.90 examples/s]"
          }
        },
        "7c48676eaa1246068486a6f1326ccc3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7df328b0855643bb9a36799182718ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808dc110997e4f83937edc38a143e69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14f4435713044a9ba9ec38e1a588ef6b",
            "max": 5702746405,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24a2aff08a0142a3b91c0be3fc1bb4b2",
            "value": 5702746405
          }
        },
        "80af76274b454071953adf69b0286079": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8348574c0088456e878d70f3389b3f45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835045ed12b44e53909ba8453aa7f624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83c80cf8d2494f8caaaa63f1d2d40eae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8462a517eb5d4772907eb3c58cf30feb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885968190880411bb6d22053a5b27f35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886c8af9ece3489d92b8e6439e69bd23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed943cafa948435a95c10da5182c60f1",
              "IPY_MODEL_bcc7501bd0ef4733bcf4821a507b66ff",
              "IPY_MODEL_43c84c1537984cfca9c5c48c0a078c07"
            ],
            "layout": "IPY_MODEL_8462a517eb5d4772907eb3c58cf30feb"
          }
        },
        "8954dafd688e47ab82e4c1f94ab68e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ac28dac0a1241dcabedf19bf0101d1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f81d60fa29e4280bbf5315c69151d30",
            "max": 1069,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b57877d0bb14f03806a145f7ec67a2a",
            "value": 1069
          }
        },
        "8af99b5146f547d1bfe1c8a1a7301b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c929d862e4b646e4819919634137bd83",
              "IPY_MODEL_fcecdb17b87d48819c948ef33fca50ff",
              "IPY_MODEL_94fb61ea675f4035bd9488adc92a6ae7"
            ],
            "layout": "IPY_MODEL_0aa621ebd40142f5888ab0f27f668ac8"
          }
        },
        "8f954e42c4074441a3ec1d0f9393c995": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94fb61ea675f4035bd9488adc92a6ae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c005a57235c6497196e59f3e3c14a5c6",
            "placeholder": "​",
            "style": "IPY_MODEL_581723738c45466da093b130a55a7b4a",
            "value": " 16.1G/16.1G [10:15&lt;00:00, 36.3MB/s]"
          }
        },
        "953886005da34a4387eaec8fbc55c475": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97037f8b83d14d848fd34cabb064bbfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b497abd5a164163928b1e83ebed2951",
            "placeholder": "​",
            "style": "IPY_MODEL_4b8e1d8a715f4d2b8ab0734f4f90a00e",
            "value": "model.safetensors: 100%"
          }
        },
        "97b54e6d916742ed9648b485858ad21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbb753733e3f4aed9d73beb25084bedf",
            "placeholder": "​",
            "style": "IPY_MODEL_00ecc999da0d44a6ac1094adabd31c6b",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 96.5kB/s]"
          }
        },
        "98584130cbf8481da7696706ee3851ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996595a5c8e746f895e76b76f34c16e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b57877d0bb14f03806a145f7ec67a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c4fc117a8374cb4abbe8233394cefd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d04661880b547e6a37ad1ad3c31e9f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e2317b56f3f48d987205b605bf8976c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c402ad9763645adb687f4a57e828e97",
              "IPY_MODEL_477c22f48991468da5d23f5253cdaa6c",
              "IPY_MODEL_7c279da6d42a4de789e8e47c67e9bd64"
            ],
            "layout": "IPY_MODEL_98584130cbf8481da7696706ee3851ed"
          }
        },
        "a034ee440b764707ae218523b4c37a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f37ecb851f42fe85a3a84709e78e80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2b093af03ae4c66bc628f51927d8160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e5b37fd30d404a91a863eea58be66c",
            "max": 4920733568,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49265549d3314062a09fbd5fd0e0922f",
            "value": 4920733568
          }
        },
        "a4552b8978e24db8b0978b767b6f325e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a69d315121c24d4d8df58666638e88c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17e7fa0037f348feab4db1f2442e6225",
            "placeholder": "​",
            "style": "IPY_MODEL_2fd5c7c1e3324124b7ffcda58bd43679",
            "value": "unsloth.Q5_K_M.gguf: 100%"
          }
        },
        "a72390e0cd604ae8bd222842e441435a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a64aff44ed4d97821e168014c39226": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c62907ab4a604f08b52c62d2f44b09ab",
            "placeholder": "​",
            "style": "IPY_MODEL_07c05e64f00c4b7ba2524b3d791f9a76",
            "value": "Downloading readme: 100%"
          }
        },
        "ac14d1761ed847b8a414e456a66ab09e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af38fd17e41448ada0e42026dc54992f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d27a633428425587f402cbedd3cdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3ba41c4fa184c9e8ab3f33f4e01ff88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7530a2f26b64f3489fd10bf15bc2139": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbb753733e3f4aed9d73beb25084bedf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc7501bd0ef4733bcf4821a507b66ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72c19ba142334e31890e0959412f0432",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78d1883404394feca2a1b746a1179d3e",
            "value": 9085698
          }
        },
        "be7964a39d0a4c0789e06f19faf5467f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_530f7170009a47129ff384a7d6ef1034",
              "IPY_MODEL_195ac429386240e49f9e56a601f61cf9",
              "IPY_MODEL_d5c10faebb2f44b6bd1431a11ae0e927"
            ],
            "layout": "IPY_MODEL_af38fd17e41448ada0e42026dc54992f"
          }
        },
        "bf114227cadd40b3900c1fe60cc1267f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4eba10188f2e4481b7cb77b7a6303684",
              "IPY_MODEL_2349e817f085499788cb7c3e41ef4844",
              "IPY_MODEL_c69695fb56af447bb3bee555b2b4d6a5"
            ],
            "layout": "IPY_MODEL_885968190880411bb6d22053a5b27f35"
          }
        },
        "bf7e337307f343c49c68d0b53aefdd63": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa39246b2be4370b911a6ee5e72e118": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c005a57235c6497196e59f3e3c14a5c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0cd6a2b57e64e7a926b97ab832a3eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c62907ab4a604f08b52c62d2f44b09ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c69695fb56af447bb3bee555b2b4d6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df328b0855643bb9a36799182718ed9",
            "placeholder": "​",
            "style": "IPY_MODEL_a4552b8978e24db8b0978b767b6f325e",
            "value": " 50.6k/50.6k [00:00&lt;00:00, 3.58MB/s]"
          }
        },
        "c717025c548f439f9deab7ba52905a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c80cf8d2494f8caaaa63f1d2d40eae",
            "placeholder": "​",
            "style": "IPY_MODEL_b1d27a633428425587f402cbedd3cdb2",
            "value": "unsloth.Q4_K_M.gguf: 100%"
          }
        },
        "c929d862e4b646e4819919634137bd83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2950f053074ff69edb5d99d1e8e0c3",
            "placeholder": "​",
            "style": "IPY_MODEL_58e7a7401cb24059b383728abd596cb4",
            "value": "unsloth.BF16.gguf: 100%"
          }
        },
        "ca2950f053074ff69edb5d99d1e8e0c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd425a2a6ff401e873a390b00236bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2ac1611ba824460b954486d9022c5d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40c76d6b420412ea41b2e62604cb416": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4fb4d6f05314372a41431013f7d0383": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c10faebb2f44b6bd1431a11ae0e927": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953886005da34a4387eaec8fbc55c475",
            "placeholder": "​",
            "style": "IPY_MODEL_28230f6db2244ef5a457b520429f83d5",
            "value": " 464/464 [00:00&lt;00:00, 41.0kB/s]"
          }
        },
        "d837e42e687f479a94bebfd10dcd42c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ae6010d2954d58bbe4b1de7998322d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a72390e0cd604ae8bd222842e441435a",
            "max": 3034923,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79d9e31ccc384903bbf1b8b8bc3af583",
            "value": 3034923
          }
        },
        "d916993dcf6347379221a523eae5104c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57c19b5efd0a4678b0fca61aa3fd84b5",
            "placeholder": "​",
            "style": "IPY_MODEL_ccd425a2a6ff401e873a390b00236bde",
            "value": " 172/172 [00:00&lt;00:00, 15.4kB/s]"
          }
        },
        "d955f14d407f4d3686ab7dd1bcdc7ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486bd0ff79514cff913afab0e37049a2",
            "placeholder": "​",
            "style": "IPY_MODEL_0a79ff85146f437aa1b825e39c772135",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "d958d6df5aec4d8eb86c07c78d21948a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b12a9b89a643f7a721b086aba667c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb7dbc75f73e48f693662da97e5ff7f3",
              "IPY_MODEL_5ea2823510574579b28aaf17250c6a70",
              "IPY_MODEL_3d6c58aea24649ec9deb416e93731bd9"
            ],
            "layout": "IPY_MODEL_4a06cf3bf52e490c8ff78a5ed0b5af8d"
          }
        },
        "db349839b0e14b03aca8fce24b40eb72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df03e34db2f54119b307180123765b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_583b91cac2444692b41f2c515d592345",
            "placeholder": "​",
            "style": "IPY_MODEL_fbd405f0c30b46a89c4d46baca78bfa4",
            "value": " 5.70G/5.70G [00:13&lt;00:00, 345MB/s]"
          }
        },
        "df9f4b1ce9984ac5b45157039873299e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1011d0c7f6149239629972063e633e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e43752904c8240a5a86f8744ee4ce025": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5a6d9d60705412abb332dd8d14e506f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e88b7aafb0d347c19be8efafbd339006": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f318178c593546e88d3235aaf666e299",
              "IPY_MODEL_08d5836446094abead951cb0332e21d9",
              "IPY_MODEL_0543c7b7b5544fc58e4eb46d67da51b8"
            ],
            "layout": "IPY_MODEL_0c8291775bbf4885b6db7a5e5fa4a7b5"
          }
        },
        "ea21abff121f41d6b4e6ed95eedfa4fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb7dbc75f73e48f693662da97e5ff7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_384b85c6341747cb8b5b4d71362dd0f2",
            "placeholder": "​",
            "style": "IPY_MODEL_58f6255e2fae4f589c2c8dcc49cf2468",
            "value": "Map (num_proc=2): 100%"
          }
        },
        "ed943cafa948435a95c10da5182c60f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c4fc117a8374cb4abbe8233394cefd9",
            "placeholder": "​",
            "style": "IPY_MODEL_ee9e3a1020544c54bd6888f0179bbc86",
            "value": "tokenizer.json: 100%"
          }
        },
        "ee9e3a1020544c54bd6888f0179bbc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f060ffe3b4e94f46ab6a32b39caf3b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c72734232014f8281fe5c6489dbdec7",
            "max": 1200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f6e6f69ba44454e8f891e885611b3f9",
            "value": 1200
          }
        },
        "f318178c593546e88d3235aaf666e299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea21abff121f41d6b4e6ed95eedfa4fe",
            "placeholder": "​",
            "style": "IPY_MODEL_73bf7a881b68469eb13472b59efc4fba",
            "value": "unsloth.Q8_0.gguf: 100%"
          }
        },
        "f322530d67cd441a91639900918b6d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35e84c97371452e94616449b1b6e215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3e5b37fd30d404a91a863eea58be66c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb171cd945294e11ab7746f4f188b580": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_996595a5c8e746f895e76b76f34c16e3",
            "placeholder": "​",
            "style": "IPY_MODEL_f322530d67cd441a91639900918b6d3a",
            "value": " 4.56k/4.56k [00:00&lt;00:00, 393kB/s]"
          }
        },
        "fbd405f0c30b46a89c4d46baca78bfa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcc224472ca241098a7eee0bb93c0f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcecdb17b87d48819c948ef33fca50ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1011d0c7f6149239629972063e633e7",
            "max": 16068890496,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0cd6a2b57e64e7a926b97ab832a3eca",
            "value": 16068890496
          }
        },
        "ffc851c5002e4690ad3b7362e776b371": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
